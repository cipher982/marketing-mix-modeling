{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159d42f0-7116-4585-aaef-a4212b68c5a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T02:00:34.651957Z",
     "iopub.status.busy": "2021-06-23T02:00:34.651792Z",
     "iopub.status.idle": "2021-06-23T02:00:35.801665Z",
     "shell.execute_reply": "2021-06-23T02:00:35.800863Z",
     "shell.execute_reply.started": "2021-06-23T02:00:34.651913Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "sns.color_palette(\"husl\")\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import pystan\n",
    "import os\n",
    "import json\n",
    "import funcs\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272c02e0-9969-4e5b-aa69-438402f4cdb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T02:00:37.354498Z",
     "iopub.status.busy": "2021-06-23T02:00:37.354302Z",
     "iopub.status.idle": "2021-06-23T02:00:39.633895Z",
     "shell.execute_reply": "2021-06-23T02:00:39.633256Z",
     "shell.execute_reply.started": "2021-06-23T02:00:37.354482Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load measured data\n",
    "m_df = pd.read_csv(\"data/measured_ad_channel_data.csv\")\n",
    "m_df = m_df[m_df[\"channel\"] != \"Congrats  YOU FOUND ME!!!!\"]\n",
    "\n",
    "# extract weekly dates\n",
    "m_df = funcs.add_week_start(m_df, 'day')\n",
    "\n",
    "# get weekly media impressions\n",
    "m_imp = m_df.groupby([\"wk_strt_dt\",\"channel\"]).sum()['impressions'].reset_index()\n",
    "m_imp = m_imp.pivot(index='wk_strt_dt', columns=['channel'], values=['impressions'])\n",
    "m_imp = m_imp.droplevel(axis=1, level=0).reset_index()\n",
    "m_imp = m_imp.fillna(value=0)\n",
    "#m_imp.head(3)\n",
    "\n",
    "# get weekly media spend\n",
    "m_sp = m_df.groupby([\"wk_strt_dt\",\"channel\"]).sum()['spend'].reset_index()\n",
    "m_sp = m_sp.pivot(index='wk_strt_dt', columns=['channel'], values=['spend'])\n",
    "m_sp = m_sp.droplevel(axis=1, level=0).reset_index()\n",
    "m_sp = m_sp.fillna(value=0)\n",
    "#m_sp.head(3)\n",
    "\n",
    "# Get weekly sales\n",
    "sales = pd.read_csv(\"data/order_data.csv.gzip\", compression=\"gzip\")\n",
    "sales['date'] = pd.to_datetime(sales['ORDER_DATE'])\n",
    "sales['weekday'] = sales['date'].dt.weekday\n",
    "sales[\"wk_strt_dt\"] = sales['date'] - sales['weekday'] * timedelta(days=1)\n",
    "sales = pd.DataFrame(sales.groupby([\"wk_strt_dt\"]).sum()['PRODUCT_SUBTOTAL'])\n",
    "sales.columns = ['sales']\n",
    "\n",
    "# Get Facebook data\n",
    "fb = pd.read_csv(\"data/collaborative_ad_data.csv\")\n",
    "fb = funcs.add_week_start(fb, \"DATE\")\n",
    "fb = pd.DataFrame(fb.groupby([\"wk_strt_dt\"])['SPEND','IMPRESSIONS'].sum())\n",
    "fb.columns = ['Facebook_spnd','Facebook_imps']\n",
    "\n",
    "# Get TV data\n",
    "tv = pd.read_csv(\"data/tv_spend.csv\")\n",
    "tv.fillna(0, inplace=True)\n",
    "tv = funcs.add_week_start(tv, 'date')\n",
    "tv['tv_imps'] = tv['spend'] / tv['cost per view']\n",
    "tv = pd.DataFrame(tv.groupby(['wk_strt_dt'])['tv_imps','spend'].sum())\n",
    "tv.columns = ['tv_imps', 'tv_spnd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a40079e-67b2-44a9-b380-7697b5f10b94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T02:00:39.634753Z",
     "iopub.status.busy": "2021-06-23T02:00:39.634620Z",
     "iopub.status.idle": "2021-06-23T02:00:39.668440Z",
     "shell.execute_reply": "2021-06-23T02:00:39.667707Z",
     "shell.execute_reply.started": "2021-06-23T02:00:39.634737Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dr = pd.date_range(start=sales.index.min(), end=sales.index.max())\n",
    "hldy_df = pd.DataFrame()\n",
    "hldy_df['date'] = dr\n",
    "\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "hldy_df['holiday'] = hldy_df['date'].isin(holidays)\n",
    "\n",
    "hldy_df = funcs.add_week_start(hldy_df, 'date')\n",
    "hldy_df = pd.DataFrame(hldy_df.groupby([\"wk_strt_dt\"]).any()['holiday'])\n",
    "hldy_df = hldy_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1289e25c-a7d8-4132-ace1-1ecaf18e8802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T02:00:39.669774Z",
     "iopub.status.busy": "2021-06-23T02:00:39.669456Z",
     "iopub.status.idle": "2021-06-23T02:00:39.698403Z",
     "shell.execute_reply": "2021-06-23T02:00:39.697746Z",
     "shell.execute_reply.started": "2021-06-23T02:00:39.669755Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge all data\n",
    "df = pd.merge(m_imp, m_sp, on='wk_strt_dt', suffixes=('_imps','_spnd'))\n",
    "df = pd.merge(df, hldy_df, left_on='wk_strt_dt', right_index=True)\n",
    "df = pd.merge(df, sales, left_on='wk_strt_dt', right_index=True)\n",
    "df = pd.merge(df, fb, left_on='wk_strt_dt', right_index=True)\n",
    "df = pd.merge(df, tv, left_on='wk_strt_dt', right_index=True, how='left')\n",
    "\n",
    "# set placeholder for seasonality\n",
    "df['seasonality'] = 1.0 \n",
    "\n",
    "# ensure no 0s in the data (bugs like them)\n",
    "df.fillna(value=0.0, inplace=True)\n",
    "df = df.replace(to_replace=0.0, value=1.0) \n",
    "\n",
    "# mean-centralize: sales, numeric base_vars\n",
    "hldy_cols = ['holiday']\n",
    "seas_cols = ['seasonality']\n",
    "me_cols = []\n",
    "st_cols = []\n",
    "mrkdn_cols = []\n",
    "\n",
    "df_ctrl, sc_ctrl = funcs.mean_center_transform(df, ['sales']+me_cols+st_cols+mrkdn_cols)\n",
    "df_ctrl = pd.concat([df_ctrl, df[hldy_cols+seas_cols]], axis=1)\n",
    "\n",
    "# variables positively related to sales: macro economy, store count, markdown, holiday\n",
    "pos_vars = ['holiday']\n",
    "X1 = df_ctrl[pos_vars].values\n",
    "\n",
    "# variables may have either positive or negtive impact on sales: seasonality\n",
    "pn_vars = [seas_cols[0]]\n",
    "X2 = df_ctrl[pn_vars].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac95a6a-42f9-4f2e-b45e-4cc94049ec9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T02:00:40.608816Z",
     "iopub.status.busy": "2021-06-23T02:00:40.608625Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_548939bc33801f8115bc26206558c913 NOW.\n",
      "INFO:pystan:OS: linux, Python: 3.8.3 (default, May 19 2020, 18:47:26) \n",
      "[GCC 7.3.0], Cython 0.29.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /tmp/pystan_l95u00d8/stanfit4anon_model_548939bc33801f8115bc26206558c913_7683304171306938440.pyx because it changed.\n",
      "[1/1] Cythonizing /tmp/pystan_l95u00d8/stanfit4anon_model_548939bc33801f8115bc26206558c913_7683304171306938440.pyx\n",
      "building 'stanfit4anon_model_548939bc33801f8115bc26206558c913_7683304171306938440' extension\n",
      "creating /tmp/pystan_l95u00d8/tmp\n",
      "creating /tmp/pystan_l95u00d8/tmp/pystan_l95u00d8\n",
      "gcc -pthread -B /home/drose/miniconda3/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DBOOST_RESULT_OF_USE_TR1 -DBOOST_NO_DECLTYPE -DBOOST_DISABLE_ASSERTS -I/tmp/pystan_l95u00d8 -I/home/drose/github/mmm_stan/.venv/lib/python3.8/site-packages/pystan -I/home/drose/github/mmm_stan/.venv/lib/python3.8/site-packages/pystan/stan/src -I/home/drose/github/mmm_stan/.venv/lib/python3.8/site-packages/pystan/stan/lib/stan_math -I/home/drose/github/mmm_stan/.venv/lib/python3.8/site-packages/pystan/stan/lib/stan_math/lib/eigen_3.3.3 -I/home/drose/github/mmm_stan/.venv/lib/python3.8/site-packages/pystan/stan/lib/stan_math/lib/boost_1.69.0 -I/home/drose/github/mmm_stan/.venv/lib/python3.8/site-packages/pystan/stan/lib/stan_math/lib/sundials_4.1.0/include -I/home/drose/github/mmm_stan/.venv/lib/python3.8/site-packages/numpy/core/include -I/home/drose/github/mmm_stan/.venv/include -I/home/drose/miniconda3/include/python3.8 -c /tmp/pystan_l95u00d8/stanfit4anon_model_548939bc33801f8115bc26206558c913_7683304171306938440.cpp -o /tmp/pystan_l95u00d8/tmp/pystan_l95u00d8/stanfit4anon_model_548939bc33801f8115bc26206558c913_7683304171306938440.o -O2 -ftemplate-depth-256 -Wno-unused-function -Wno-uninitialized -std=c++1y\n"
     ]
    }
   ],
   "source": [
    "ctrl_data = {\n",
    "    'N': len(df_ctrl),\n",
    "    'K1': len(pos_vars), \n",
    "    'K2': len(pn_vars), \n",
    "    'X1': X1,\n",
    "    'X2': X2, \n",
    "    'y': df_ctrl['sales'].values,\n",
    "    'max_intercept': min(df_ctrl['sales'])\n",
    "}\n",
    "\n",
    "ctrl_code1 = '''\n",
    "data {\n",
    "  int N; // number of observations\n",
    "  int K1; // number of positive predictors\n",
    "  int K2; // number of positive/negative predictors\n",
    "  real max_intercept; // restrict the intercept to be less than the minimum y\n",
    "  matrix[N, K1] X1;\n",
    "  matrix[N, K2] X2;\n",
    "  vector[N] y; \n",
    "}\n",
    "\n",
    "parameters {\n",
    "  vector<lower=0>[K1] beta1; // regression coefficients for X1 (positive)\n",
    "  vector[K2] beta2; // regression coefficients for X2\n",
    "  real<lower=0, upper=max_intercept> alpha; // intercept\n",
    "  real<lower=0> noise_var; // residual variance\n",
    "}\n",
    "\n",
    "model {\n",
    "  // Define the priors\n",
    "  beta1 ~ normal(0, 1); \n",
    "  beta2 ~ normal(0, 1); \n",
    "  noise_var ~ inv_gamma(0.05, 0.05 * 0.01);\n",
    "  // The likelihood\n",
    "  y ~ normal(X1*beta1 + X2*beta2 + alpha, sqrt(noise_var));\n",
    "}\n",
    "'''\n",
    "try:\n",
    "    sm1\n",
    "except NameError:\n",
    "    sm1 = pystan.StanModel(model_code=ctrl_code1, verbose=True)\n",
    "\n",
    "fit1 = sm1.sampling(data=ctrl_data, iter=2000, chains=4)\n",
    "fit1_result = fit1.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e2af5-e3f8-48ef-87f5-cf500f9f0498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp_cols = [col for col in df.columns if '_spnd' in col]\n",
    "spend_df = df[sp_cols + ['sales']]\n",
    "\n",
    "# Setting X and y variables\n",
    "X = spend_df.loc[:, spend_df.columns != 'sales']\n",
    "y = spend_df['sales']\n",
    "# Building Random Forest model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
    "model = RandomForestRegressor(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "# Visualizing Feature Importance\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(25).plot(kind='barh',figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39208db-7b7f-478d-91ec-d44369137131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract control model parameters and predict base sales -> df['base_sales']\n",
    "def extract_ctrl_model(fit_result, pos_vars=pos_vars, pn_vars=pn_vars, \n",
    "                       extract_param_list=False):\n",
    "    ctrl_model = {}\n",
    "    ctrl_model['pos_vars'] = pos_vars\n",
    "    ctrl_model['pn_vars'] = pn_vars\n",
    "    ctrl_model['beta1'] = fit_result['beta1'].mean(axis=0).tolist()\n",
    "    ctrl_model['beta2'] = fit_result['beta2'].mean(axis=0).tolist()\n",
    "    ctrl_model['alpha'] = fit_result['alpha'].mean()\n",
    "    if extract_param_list:\n",
    "        ctrl_model['beta1_list'] = fit_result['beta1'].tolist()\n",
    "        ctrl_model['beta2_list'] = fit_result['beta2'].tolist()\n",
    "        ctrl_model['alpha_list'] = fit_result['alpha'].tolist()\n",
    "    return ctrl_model\n",
    "\n",
    "def ctrl_model_predict(ctrl_model, df):\n",
    "    pos_vars, pn_vars = ctrl_model['pos_vars'], ctrl_model['pn_vars'] \n",
    "    X1, X2 = df[pos_vars], df[pn_vars]\n",
    "    beta1, beta2 = np.array(ctrl_model['beta1']), np.array(ctrl_model['beta2'])\n",
    "    alpha = ctrl_model['alpha']\n",
    "    y_pred = np.dot(X1, beta1) + np.dot(X2, beta2) + alpha\n",
    "    return y_pred\n",
    "\n",
    "base_sales_model = extract_ctrl_model(fit1_result, pos_vars=pos_vars, pn_vars=pn_vars)\n",
    "base_sales = ctrl_model_predict(base_sales_model, df_ctrl)\n",
    "df['base_sales'] = base_sales*sc_ctrl['sales']\n",
    "# evaluate control model\n",
    "print('mape: ', funcs.mean_absolute_percentage_error(df['sales'], df['base_sales']))\n",
    "\n",
    "# np.savetxt(\"base_sales_pred.csv\", df['base_sales'].values, delimiter=\",\")\n",
    "# save_json(base_sales_model, 'ctrl_model.json')\n",
    "# df['base_sales'] = pd.read_csv('base_sales_pred.csv', header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb354e-6517-4289-b124-853727c219b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.2 Marketing Mix Model\n",
    "mdip_cols = [col for col in df.columns if '_imp' in col]\n",
    "mdip_cols\n",
    "\n",
    "df_mmm, sc_mmm = funcs.mean_log1p_trandform(df, ['sales', 'base_sales'])\n",
    "mu_mdip = df[mdip_cols].apply(np.mean, axis=0).values\n",
    "mu_mdip[1] = 100\n",
    "max_lag = 4\n",
    "num_media = len(mdip_cols)\n",
    "# padding zero * (max_lag-1) rows\n",
    "X_media = np.concatenate((np.zeros((max_lag-1, num_media)), df[mdip_cols].values), axis=0)\n",
    "X_media = np.nan_to_num(X_media)\n",
    "X_ctrl = df_mmm['base_sales'].values.reshape(len(df),1)\n",
    "model_data2 = {\n",
    "    'N': len(df),\n",
    "    'max_lag': max_lag, \n",
    "    'num_media': num_media,\n",
    "    'X_media': X_media, \n",
    "    'mu_mdip': mu_mdip,\n",
    "    'num_ctrl': X_ctrl.shape[1],\n",
    "    'X_ctrl': X_ctrl, \n",
    "    'y': df_mmm['sales'].values\n",
    "}\n",
    "\n",
    "model_code2 = '''\n",
    "functions {\n",
    "  // the adstock transformation with a vector of weights\n",
    "  real Adstock(vector t, row_vector weights) {\n",
    "    return dot_product(t, weights) / sum(weights);\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  // the total number of observations\n",
    "  int<lower=1> N;\n",
    "  // the vector of sales\n",
    "  real y[N];\n",
    "  // the maximum duration of lag effect, in weeks\n",
    "  int<lower=1> max_lag;\n",
    "  // the number of media channels\n",
    "  int<lower=1> num_media;\n",
    "  // matrix of media variables\n",
    "  matrix[N+max_lag-1, num_media] X_media;\n",
    "  // vector of media variables' mean\n",
    "  real mu_mdip[num_media];\n",
    "  // the number of other control variables\n",
    "  int<lower=1> num_ctrl;\n",
    "  // a matrix of control variables\n",
    "  matrix[N, num_ctrl] X_ctrl;\n",
    "}\n",
    "parameters {\n",
    "  // residual variance\n",
    "  real<lower=0> noise_var;\n",
    "  // the intercept\n",
    "  real tau;\n",
    "  // the coefficients for media variables and base sales\n",
    "  vector<lower=0>[num_media+num_ctrl] beta;\n",
    "  // the decay and peak parameter for the adstock transformation of\n",
    "  // each media\n",
    "  vector<lower=0,upper=1>[num_media] decay;\n",
    "  vector<lower=0,upper=ceil(max_lag/2)>[num_media] peak;\n",
    "}\n",
    "transformed parameters {\n",
    "  // the cumulative media effect after adstock\n",
    "  real cum_effect;\n",
    "  // matrix of media variables after adstock\n",
    "  matrix[N, num_media] X_media_adstocked;\n",
    "  // matrix of all predictors\n",
    "  matrix[N, num_media+num_ctrl] X;\n",
    "  \n",
    "  // adstock, mean-center, log1p transformation\n",
    "  row_vector[max_lag] lag_weights;\n",
    "  for (nn in 1:N) {\n",
    "    for (media in 1 : num_media) {\n",
    "      for (lag in 1 : max_lag) {\n",
    "        lag_weights[max_lag-lag+1] <- pow(decay[media], (lag - 1 - peak[media]) ^ 2);\n",
    "      }\n",
    "     cum_effect <- Adstock(sub_col(X_media, nn, media, max_lag), lag_weights);\n",
    "     X_media_adstocked[nn, media] <- log1p(cum_effect/mu_mdip[media]);\n",
    "    }\n",
    "  X <- append_col(X_media_adstocked, X_ctrl);\n",
    "  } \n",
    "}\n",
    "model {\n",
    "  decay ~ beta(3,3);\n",
    "  peak ~ uniform(0, ceil(max_lag/2));\n",
    "  tau ~ normal(0, 5);\n",
    "  for (i in 1 : num_media+num_ctrl) {\n",
    "    beta[i] ~ normal(0, 1);\n",
    "  }\n",
    "  noise_var ~ inv_gamma(0.05, 0.05 * 0.01);\n",
    "  y ~ normal(tau + X * beta, sqrt(noise_var));\n",
    "}\n",
    "'''\n",
    "print(\"Loading StanModel. . .\")\n",
    "try:\n",
    "    sm2\n",
    "except NameError:\n",
    "    sm2 = pystan.StanModel(model_code=model_code2, verbose=True)\n",
    "#sm2 = pystan.StanModel(model_code=model_code2, verbose=True)\n",
    "print(\"Beginning sampling. . .\")\n",
    "fit2 = sm2.sampling(data=model_data2, iter=1000, chains=3)\n",
    "fit2_result = fit2.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b836bc8-df11-41af-ad92-26e2b5f19932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mmm = funcs.extract_mmm(\n",
    "    fit2, \n",
    "    max_lag=max_lag, \n",
    "    media_vars=mdip_cols, \n",
    "    ctrl_vars=['base_sales']\n",
    ")\n",
    "\n",
    "# save_json(mmm, 'mmm1.json')\n",
    "# plot media coefficients' distributions\n",
    "# red line: mean, green line: median\n",
    "beta_media = {}\n",
    "for i in range(len(mmm['media_vars'])):\n",
    "    md = mmm['media_vars'][i]\n",
    "    betas = []\n",
    "    for j in range(len(mmm['beta_list'])):\n",
    "        betas.append(mmm['beta_list'][j][i])\n",
    "    beta_media[md] = np.array(betas)\n",
    "\n",
    "f = plt.figure(figsize=(18,15))\n",
    "for i in range(len(mmm['media_vars'])):\n",
    "    ax = f.add_subplot(5,3,i+1)\n",
    "    md = mmm['media_vars'][i]\n",
    "    x = beta_media[md]\n",
    "    mean_x = x.mean()\n",
    "    median_x = np.median(x)\n",
    "    ax = sns.distplot(x)\n",
    "    ax.axvline(mean_x, color='r', linestyle='-')\n",
    "    ax.axvline(median_x, color='g', linestyle='-')\n",
    "    ax.set_title(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da3d10-9757-4b58-82f2-c0c8700f6e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decompose sales to media contribution\n",
    "mc_df = funcs.mmm_decompose_contrib(mmm, df, original_sales=df['sales'])\n",
    "adstock_params = mmm['adstock_params']\n",
    "\n",
    "# calculate media contribution percentage\n",
    "mc_pct, mc_pct2 = funcs.calc_media_contrib_pct(mc_df, mdip_cols, 'sales', period=52)\n",
    "\n",
    "# mc_df.to_csv('mc_df1.csv', index=False)\n",
    "# save_json(adstock_params, 'adstock_params1.json')\n",
    "# pd.concat([\n",
    "#     pd.DataFrame.from_dict(mc_pct, orient='index', columns=['mc_pct']),\n",
    "#     pd.DataFrame.from_dict(mc_pct2, orient='index', columns=['mc_pct2'])\n",
    "# ], axis=1).to_csv('mc_pct_df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb535add-b573-4ecd-b3fe-1195e45b320f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_code3 = '''\n",
    "functions {\n",
    "  // the Hill function\n",
    "  real Hill(real t, real ec, real slope) {\n",
    "  return 1 / (1 + (t / ec)^(-slope));\n",
    "  }\n",
    "}\n",
    "\n",
    "data {\n",
    "  // the total number of observations\n",
    "  int<lower=1> N;\n",
    "  // y: vector of media contribution\n",
    "  vector[N] y;\n",
    "  // X: vector of adstocked media spending\n",
    "  vector[N] X;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  // residual variance\n",
    "  real<lower=0> noise_var;\n",
    "  // regression coefficient\n",
    "  real<lower=0> beta_hill;\n",
    "  // ec50 and slope for Hill function of the media\n",
    "  real<lower=0,upper=1> ec;\n",
    "  real<lower=0> slope;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "  // a vector of the mean response\n",
    "  vector[N] mu;\n",
    "  for (i in 1:N) {\n",
    "    mu[i] <- beta_hill * Hill(X[i], ec, slope);\n",
    "  }\n",
    "}\n",
    "\n",
    "model {\n",
    "  slope ~ gamma(3, 1);\n",
    "  ec ~ beta(2, 2);\n",
    "  beta_hill ~ normal(0, 1);\n",
    "  noise_var ~ inv_gamma(0.05, 0.05 * 0.01); \n",
    "  y ~ normal(mu, sqrt(noise_var));\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2fedf-ff2a-4ad4-a6e7-351f57c16732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train hill models for all media channels\n",
    "try:\n",
    "    sm3\n",
    "except NameError:\n",
    "    sm3 = pystan.StanModel(model_code=model_code3, verbose=True)\n",
    "sm3 = pystan.StanModel(model_code=model_code3, verbose=True)\n",
    "hill_models = {}\n",
    "#to_train = ['dm', 'inst', 'nsp', 'auddig', 'audtr', 'vidtr', 'viddig', 'so', 'on', 'sem']\n",
    "to_train = ['ASA', 'Affiliate', 'Display', 'Email', 'PLA', \n",
    "            'Search', 'Social', 'Video', 'Facebook', 'tv']\n",
    "for media in to_train:\n",
    "    print('training for media: ', media)\n",
    "    hill_model = funcs.train_hill_model(df, mc_df, adstock_params, media, sm3)\n",
    "    print(\"trained for media: \", media)\n",
    "    hill_models[media] = hill_model\n",
    "\n",
    "# extract params by mean\n",
    "hill_model_params_mean, hill_model_params_med = {}, {}\n",
    "for md in list(hill_models.keys()):\n",
    "    print(\"extracting \" + md)\n",
    "    hill_model = hill_models[md]\n",
    "    params1 = funcs.extract_hill_model_params(hill_model, method='mean')\n",
    "    params1['sc'] = hill_model['sc']\n",
    "    hill_model_params_mean[md] = params1\n",
    "#     params2 = extract_hill_model_params(hill_model, method='median')\n",
    "#     params2['sc'] = hill_model['sc']\n",
    "#     hill_model_params_med[md] = params2\n",
    "# save_json(hill_model_params_med, 'hill_model_params_med.json')\n",
    "# save_json(hill_model_params_mean, 'hill_model_params_mean.json')\n",
    "\n",
    "# evaluate model params extracted by mean\n",
    "for md in list(hill_models.keys()):\n",
    "    print('evaluating media: ', md)\n",
    "    hill_model = hill_models[md]\n",
    "    hill_model_params = hill_model_params_mean[md]\n",
    "    _ = funcs.evaluate_hill_model(hill_model, hill_model_params)\n",
    "# evaluate model params extracted by median\n",
    "# for md in list(hill_models.keys()):\n",
    "#     print('evaluating media: ', md)\n",
    "#     hill_model = hill_models[md]\n",
    "#     hill_model_params = hill_model_params_med[md]\n",
    "#     _ = evaluate_hill_model(hill_model, hill_model_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736e486-f7a6-4fa5-a2c6-2cccb1839879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate overall ROAS and weekly ROAS\n",
    "# - Overall ROAS = total contribution / total spending\n",
    "# - Weekly ROAS = weekly contribution / weekly spending\n",
    "\n",
    "# adstocked media spending\n",
    "ms_df = pd.DataFrame()\n",
    "for md in list(hill_models.keys()):\n",
    "    #print(\"md: \",md)\n",
    "    hill_model = hill_models[md]\n",
    "    x = np.array(hill_model['data']['X']) * hill_model['sc']['x']\n",
    "    ms_df[md+\"_spnd\"] = x\n",
    "# ms_df.to_csv('ms_df1.csv', index=False)\n",
    "\n",
    "roas_1y = funcs.calc_roas(mc_df, ms_df, period=52)\n",
    "weekly_roas = funcs.calc_weekly_roas(mc_df, ms_df)\n",
    "roas1y_df = pd.DataFrame(index=weekly_roas.columns.tolist())\n",
    "roas1y_df['roas_mean'] = weekly_roas[-52:].apply(np.mean, axis=0)\n",
    "roas1y_df['roas_median'] = weekly_roas[-52:].apply(np.median, axis=0)\n",
    "\n",
    "\n",
    "# # plot weekly ROAS distribution\n",
    "# # median: green line, mean: red line\n",
    "# f = plt.figure(figsize=(18,12))\n",
    "# for i in range(len(weekly_roas.columns)):\n",
    "#     md = weekly_roas.columns[i]\n",
    "#     ax = f.add_subplot(4,3,i+1)\n",
    "#     x = weekly_roas[md]\n",
    "#     mean_x = np.mean(x)\n",
    "#     median_x = np.median(x)\n",
    "#     ax = sns.distplot(x)\n",
    "#     ax.axvline(mean_x, color='r', linestyle='-', alpha=0.5)\n",
    "#     ax.axvline(median_x, color='g', linestyle='-', alpha=0.5)\n",
    "#     ax.set(xlabel=None)\n",
    "#     ax.set_title(md)\n",
    "\n",
    "# plot weekly ROAS distribution of past 1 year\n",
    "# median: green line, mean: red line\n",
    "f = plt.figure(figsize=(18,12))\n",
    "for i in range(len(weekly_roas.columns)):\n",
    "    md = weekly_roas.columns[i]\n",
    "    ax = f.add_subplot(4,3,i+1)\n",
    "    x = weekly_roas[md][-52:]\n",
    "    mean_x = np.mean(x)\n",
    "    median_x = np.median(x)\n",
    "    ax = sns.distplot(x)\n",
    "    ax.axvline(mean_x, color='r', linestyle='-', alpha=0.5)\n",
    "    ax.axvline(median_x, color='g', linestyle='-', alpha=0.5)\n",
    "    ax.set(xlabel=None)\n",
    "    ax.set_title(md)\n",
    "\n",
    "\n",
    "# Calculate mROAS\n",
    "# 1. Current spending level (cur_sp) is represented by mean or median of weekly spending.    \n",
    "# Next spending level (next_sp) is increasing cur_sp by 1%.\n",
    "# 2. Plug cur_sp and next_sp into the Hill function:    \n",
    "# Current media contribution: cur_mc = Hill(cur_sp)    \n",
    "# Next-level media contribution next_mc = Hill(next_sp)    \n",
    "# 3. mROAS = (next_mc - cur_mc) / (0.01 * cur_sp)\n",
    "\n",
    "\n",
    "# calc mROAS of recent 1 year\n",
    "mroas_1y = {}\n",
    "for md in list(hill_models.keys()):\n",
    "    hill_model = hill_models[md]\n",
    "    hill_model_params = hill_model_params_mean[md]\n",
    "    mroas_1y[md] = funcs.calc_mroas(hill_model, hill_model_params, period=52)\n",
    "\n",
    "roas1y_df = pd.concat([\n",
    "    roas1y_df[['roas_mean', 'roas_median']],\n",
    "    pd.DataFrame.from_dict(mroas_1y, orient='index', columns=['mroas']),\n",
    "    pd.DataFrame.from_dict(roas_1y, orient='index', columns=['roas_avg'])\n",
    "], axis=1)\n",
    "# roas1y_df.to_csv('roas1y_df1.csv')\n",
    "\n",
    "roas1y_df\n",
    "# **ROAS & mROAS**    \n",
    "# 'roas_avg': overall ROAS = total contribution / total spending    \n",
    "# 'roas_mean': mean of weekly ROAS    \n",
    "# 'roas_median': median of weekly ROAS    \n",
    "# 'mroas': mROAS calculated based on increasing current spending level by 1%   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727d742-2a3c-4c67-825c-5c9a670df239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353dbf4-edea-43f5-9c5a-d7fee716483b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
