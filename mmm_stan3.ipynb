{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7dd57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQyvd_-Qth6U",
    "outputId": "5b641f0d-d580-454d-86b5-bb8b4dea79f7"
   },
   "outputs": [],
   "source": [
    "# Clone repo if running from Colab\n",
    "\n",
    "#!git clone https://github.com/cipher982/marketing-mix-modeling.git\n",
    "#!cp marketing-mix-modeling/funcs.py funcs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2dd8ca-ca0d-4588-9378-a049a2bff040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-07T21:46:30.385204Z",
     "iopub.status.busy": "2021-07-07T21:46:30.384850Z",
     "iopub.status.idle": "2021-07-07T21:46:30.396805Z",
     "shell.execute_reply": "2021-07-07T21:46:30.396086Z",
     "shell.execute_reply.started": "2021-07-07T21:46:30.385188Z"
    }
   },
   "source": [
    "# Setup\n",
    "## Import and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ceb5c0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-07T21:42:29.196041Z",
     "iopub.status.busy": "2021-07-07T21:42:29.195660Z",
     "iopub.status.idle": "2021-07-07T21:42:29.939568Z",
     "shell.execute_reply": "2021-07-07T21:42:29.938894Z",
     "shell.execute_reply.started": "2021-07-07T21:42:29.195989Z"
    },
    "id": "159d42f0-7116-4585-aaef-a4212b68c5a1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "sns.color_palette(\"husl\")\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# import pystan\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import stan\n",
    "import os\n",
    "import json\n",
    "import funcs\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b855ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-07T21:46:57.289424Z",
     "iopub.status.busy": "2021-07-07T21:46:57.289074Z",
     "iopub.status.idle": "2021-07-07T21:46:57.302218Z",
     "shell.execute_reply": "2021-07-07T21:46:57.301525Z",
     "shell.execute_reply.started": "2021-07-07T21:46:57.289406Z"
    },
    "id": "ZnFaGwxAOWNb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/'\n",
    "NUM_CHAINS = 12\n",
    "SEED = 0\n",
    "\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a5132",
   "metadata": {
    "id": "dasjQPH4PlUM"
   },
   "source": [
    "# Data Prep\n",
    "## Load and Group Data\n",
    "Load and re-format to a weekly grouping the datasets provided:\n",
    "- Measured ad channels \n",
    "- Product sales\n",
    "- Facebook\n",
    "- TV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7077f7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-07-07T21:42:33.890510Z",
     "iopub.status.busy": "2021-07-07T21:42:33.890197Z",
     "iopub.status.idle": "2021-07-07T21:42:35.797106Z",
     "shell.execute_reply": "2021-07-07T21:42:35.796251Z",
     "shell.execute_reply.started": "2021-07-07T21:42:33.890494Z"
    },
    "id": "272c02e0-9969-4e5b-aa69-438402f4cdb1",
    "outputId": "536a3f43-217d-4d8f-f5ce-2a3d8c3cadfc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all datasets.\n",
      "CPU times: user 1.76 s, sys: 128 ms, total: 1.89 s\n",
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load measured data\n",
    "m_df = pd.read_csv(DATA_DIR+\"measured_ad_channel_data.csv\")\n",
    "m_df = m_df[m_df[\"channel\"] != \"Congrats  YOU FOUND ME!!!!\"]\n",
    "\n",
    "# extract weekly dates\n",
    "m_df = funcs.add_week_start(m_df, 'day')\n",
    "\n",
    "# get weekly media impressions\n",
    "m_imp = m_df.groupby([\"wk_strt_dt\",\"channel\"]).sum()['impressions'].reset_index()\n",
    "m_imp = m_imp.pivot(index='wk_strt_dt', columns=['channel'], values=['impressions'])\n",
    "m_imp = m_imp.droplevel(axis=1, level=0).reset_index()\n",
    "m_imp = m_imp.fillna(value=0)\n",
    "#m_imp.head(3)\n",
    "\n",
    "# get weekly media spend\n",
    "m_sp = m_df.groupby([\"wk_strt_dt\",\"channel\"]).sum()['spend'].reset_index()\n",
    "m_sp = m_sp.pivot(index='wk_strt_dt', columns=['channel'], values=['spend'])\n",
    "m_sp = m_sp.droplevel(axis=1, level=0).reset_index()\n",
    "m_sp = m_sp.fillna(value=0)\n",
    "#m_sp.head(3)\n",
    "\n",
    "# Get weekly sales\n",
    "sales = pd.read_csv(DATA_DIR+\"order_data.csv.gzip\", compression=\"gzip\")\n",
    "sales['date'] = pd.to_datetime(sales['ORDER_DATE'])\n",
    "sales['weekday'] = sales['date'].dt.weekday\n",
    "sales[\"wk_strt_dt\"] = sales['date'] - sales['weekday'] * timedelta(days=1)\n",
    "sales = pd.DataFrame(sales.groupby([\"wk_strt_dt\"]).sum()['PRODUCT_SUBTOTAL'])\n",
    "sales.columns = ['sales']\n",
    "\n",
    "# Get Facebook data\n",
    "fb = pd.read_csv(DATA_DIR+\"collaborative_ad_data.csv\")\n",
    "fb = funcs.add_week_start(fb, \"DATE\")\n",
    "fb = pd.DataFrame(fb.groupby([\"wk_strt_dt\"])['SPEND','IMPRESSIONS'].sum())\n",
    "fb.columns = ['Facebook_spnd','Facebook_imps']\n",
    "\n",
    "# Get TV data\n",
    "tv = pd.read_csv(DATA_DIR+\"tv_spend.csv\")\n",
    "tv.fillna(0, inplace=True)\n",
    "tv = funcs.add_week_start(tv, 'date')\n",
    "tv['tv_imps'] = tv['spend'] / tv['cost per view']\n",
    "tv = pd.DataFrame(tv.groupby(['wk_strt_dt'])['tv_imps','spend'].sum())\n",
    "tv.columns = ['tv_imps', 'tv_spnd']\n",
    "\n",
    "# Create holiday data\n",
    "dr = pd.date_range(start=sales.index.min(), end=sales.index.max())\n",
    "hldy_df = pd.DataFrame()\n",
    "hldy_df['date'] = dr\n",
    "\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "hldy_df['holiday'] = hldy_df['date'].isin(holidays)\n",
    "\n",
    "hldy_df = funcs.add_week_start(hldy_df, 'date')\n",
    "hldy_df = pd.DataFrame(hldy_df.groupby([\"wk_strt_dt\"]).any()['holiday'])\n",
    "hldy_df = hldy_df.astype(int)\n",
    "print(\"Loaded all datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2a85a9",
   "metadata": {
    "id": "RXi5mUdhdWQg"
   },
   "source": [
    "## Combine Data\n",
    "\n",
    "Merge the datasets together for a master df. Also add in control data for holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442c7c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-07T21:42:35.798795Z",
     "iopub.status.busy": "2021-07-07T21:42:35.798420Z",
     "iopub.status.idle": "2021-07-07T21:42:35.821426Z",
     "shell.execute_reply": "2021-07-07T21:42:35.820756Z",
     "shell.execute_reply.started": "2021-07-07T21:42:35.798776Z"
    },
    "id": "1289e25c-a7d8-4132-ace1-1ecaf18e8802",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created master DataFrame\n",
      "CPU times: user 10.2 ms, sys: 38 Âµs, total: 10.2 ms\n",
      "Wall time: 9.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Merge all data\n",
    "df = pd.merge(m_imp, m_sp, on='wk_strt_dt', suffixes=('_imps','_spnd'))\n",
    "df = pd.merge(df, hldy_df, left_on='wk_strt_dt', right_index=True)\n",
    "df = pd.merge(df, sales, left_on='wk_strt_dt', right_index=True)\n",
    "df = pd.merge(df, fb, left_on='wk_strt_dt', right_index=True)\n",
    "df = pd.merge(df, tv, left_on='wk_strt_dt', right_index=True, how='left')\n",
    "\n",
    "# set placeholder for seasonality\n",
    "df['seasonality'] = 1.0 \n",
    "\n",
    "# ensure no 0s in the data (bugs like them)\n",
    "df.fillna(value=0.0, inplace=True)\n",
    "df = df.replace(to_replace=0.0, value=1.0) \n",
    "\n",
    "# mean-centralize: sales, numeric base_vars\n",
    "hldy_cols = ['holiday']\n",
    "seas_cols = ['seasonality']\n",
    "me_cols = []\n",
    "st_cols = []\n",
    "mrkdn_cols = []\n",
    "\n",
    "df_ctrl, sc_ctrl = funcs.mean_center_transform(\n",
    "    df=df, \n",
    "    cols=['sales']+me_cols+st_cols+mrkdn_cols\n",
    ")\n",
    "df_ctrl = pd.concat([df_ctrl, df[hldy_cols+seas_cols]], axis=1)\n",
    "\n",
    "# variables with pos effect on sales: economy, num_stores, sales, holiday\n",
    "pos_vars = ['holiday']\n",
    "X1 = df_ctrl[pos_vars].values\n",
    "\n",
    "# variables may have either pos or neg impact on sales: seasonality\n",
    "pn_vars = [seas_cols[0]]\n",
    "X2 = df_ctrl[pn_vars].values\n",
    "\n",
    "print(\"Created master DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea292aa-1ef5-4067-802e-26f097d8a0c5",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## STAN Model (control)\n",
    "\n",
    "Create our first simple STAN model based off just the control data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03673fd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-07-07T21:43:37.086713Z",
     "iopub.status.busy": "2021-07-07T21:43:37.086366Z",
     "iopub.status.idle": "2021-07-07T21:43:40.378175Z",
     "shell.execute_reply": "2021-07-07T21:43:40.377357Z",
     "shell.execute_reply.started": "2021-07-07T21:43:37.086694Z"
    },
    "id": "3ac95a6a-42f9-4f2e-b45e-4cc94049ec9a",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6afc9ce5-13fa-433f-adc2-e4c71cceeea0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mBuilding:\u001b[0m found in cache, done.\n",
      "\u001b[36mMessages from \u001b[0m\u001b[36;1mstanc\u001b[0m\u001b[36m:\u001b[0m\n",
      "Warning in '/tmp/httpstan_390u6zhc/model_dpdspt3f.stan', line 23, column 30: Argument 0.0005 suggests there may be parameters that are not unit scale; consider rescaling with a multiplier (see manual section 22.12).\n",
      "Warning in '/tmp/httpstan_390u6zhc/model_dpdspt3f.stan', line 23, column 24: Argument 0.05 suggests there may be parameters that are not unit scale; consider rescaling with a multiplier (see manual section 22.12).\n",
      "Warning: The parameter alpha has no priors.\n",
      "\u001b[36mSampling:\u001b[0m   0%\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m   8% (1500/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  17% (3000/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  25% (4500/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  33% (6000/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  42% (7500/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  50% (9000/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  58% (10500/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  67% (12000/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  75% (13500/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  83% (15000/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  92% (16500/18000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m 100% (18000/18000)\n",
      "\u001b[1A\u001b[0J\u001b[32mSampling:\u001b[0m 100% (18000/18000), done.\n",
      "\u001b[36mMessages received during sampling:\u001b[0m\n",
      "  Gradient evaluation took 4e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/httpstan_gbku7oke/model_dpdspt3f.stan', line 25, column 2 to column 59)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 3.7e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 3.8e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: normal_lpdf: Location parameter[1] is inf, but must be finite! (in '/tmp/httpstan_gbku7oke/model_dpdspt3f.stan', line 25, column 2 to column 59)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 3.8e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 3.6e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 4.3e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: normal_lpdf: Location parameter[1] is inf, but must be finite! (in '/tmp/httpstan_gbku7oke/model_dpdspt3f.stan', line 25, column 2 to column 59)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 3.7e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 3.6e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 3.9e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: normal_lpdf: Location parameter[1] is inf, but must be finite! (in '/tmp/httpstan_gbku7oke/model_dpdspt3f.stan', line 25, column 2 to column 59)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 4e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/httpstan_gbku7oke/model_dpdspt3f.stan', line 25, column 2 to column 59)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 4.1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/httpstan_gbku7oke/model_dpdspt3f.stan', line 25, column 2 to column 59)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 3.7e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 471 ms, sys: 217 ms, total: 688 ms\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ctrl_data = {\n",
    "    'N': len(df_ctrl), # dataset\n",
    "    'K1': len(pos_vars), # len of positive only features\n",
    "    'K2': len(pn_vars), # len of pos or neg features\n",
    "    'X1': X1, # positive features values\n",
    "    'X2': X2, # pos or neg features values\n",
    "    'y': df_ctrl['sales'].values, # sales as label\n",
    "    'max_intercept': min(df_ctrl['sales'])\n",
    "}\n",
    "\n",
    "ctrl_code1 = '''\n",
    "data {\n",
    "  int N; // number of observations\n",
    "  int K1; // number of positive predictors\n",
    "  int K2; // number of positive/negative predictors\n",
    "  real max_intercept; // restrict the intercept to be less than the minimum y\n",
    "  matrix[N, K1] X1;\n",
    "  matrix[N, K2] X2;\n",
    "  vector[N] y; \n",
    "}\n",
    "\n",
    "parameters {\n",
    "  vector<lower=0>[K1] beta1; // regression coefficients for X1 (positive)\n",
    "  vector[K2] beta2; // regression coefficients for X2\n",
    "  real<lower=0, upper=max_intercept> alpha; // intercept\n",
    "  real<lower=0> noise_var; // residual variance\n",
    "}\n",
    "\n",
    "model {\n",
    "  // Define the priors\n",
    "  beta1 ~ normal(0, 1); \n",
    "  beta2 ~ normal(0, 1); \n",
    "  noise_var ~ inv_gamma(0.05, 0.05 * 0.01);\n",
    "  // The likelihood\n",
    "  y ~ normal(X1*beta1 + X2*beta2 + alpha, sqrt(noise_var));\n",
    "}\n",
    "'''\n",
    "post_ctrl = stan.build(\n",
    "    program_code=ctrl_code1, \n",
    "    data=ctrl_data, \n",
    "    random_seed=SEED\n",
    ")\n",
    "\n",
    "fit_ctrl = post_ctrl.sample(\n",
    "    num_chains=NUM_CHAINS, \n",
    "    num_samples=500\n",
    ")\n",
    "\n",
    "fit_ctrl_result = fit_ctrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df349701-726d-4b40-9429-0f5f9e335268",
   "metadata": {},
   "source": [
    "## Extract and Predict\n",
    "Pull the results from the STAN model and predict a level of base sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "081b0f60-45d7-4f5e-96db-7a4326344263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-07T22:04:55.438156Z",
     "iopub.status.busy": "2021-07-07T22:04:55.437837Z",
     "iopub.status.idle": "2021-07-07T22:04:55.450693Z",
     "shell.execute_reply": "2021-07-07T22:04:55.450013Z",
     "shell.execute_reply.started": "2021-07-07T22:04:55.438137Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(<stan.Fit>\n",
       "Parameters:\n",
       "    beta1: (1,)\n",
       "    beta2: (1,)\n",
       "    alpha: ()\n",
       "    noise_var: ()\n",
       "Draws: 6000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_ctrl_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80b1d03c-1199-417e-9a48-2765a7f8c2b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-07T22:01:07.526584Z",
     "iopub.status.busy": "2021-07-07T22:01:07.526259Z",
     "iopub.status.idle": "2021-07-07T22:01:07.540553Z",
     "shell.execute_reply": "2021-07-07T22:01:07.539909Z",
     "shell.execute_reply.started": "2021-07-07T22:01:07.526569Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ctrl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2b1324c-dbb7-48d4-b677-282de8a5bcc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-07T22:07:28.097094Z",
     "iopub.status.busy": "2021-07-07T22:07:28.096850Z",
     "iopub.status.idle": "2021-07-07T22:07:28.123177Z",
     "shell.execute_reply": "2021-07-07T22:07:28.122322Z",
     "shell.execute_reply.started": "2021-07-07T22:07:28.097075Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158, 1)\n",
      "[0.07075783 1.1677583  0.62728866 ... 1.66095706 2.15018691 1.17940757]\n",
      "(158, 1)\n",
      "(6000,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (158,1) and (6000,) not aligned: 1 (dim 1) != 6000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3097/1393378124.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbase_sales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctrl_model_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_sales_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3097/1393378124.py\u001b[0m in \u001b[0;36mctrl_model_predict\u001b[0;34m(ctrl_model, df)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (158,1) and (6000,) not aligned: 1 (dim 1) != 6000 (dim 0)"
     ]
    }
   ],
   "source": [
    "def ctrl_model_predict(ctrl_model, df):\n",
    "    pos_vars, pn_vars = ctrl_model['pos_vars'], ctrl_model['pn_vars'] \n",
    "    X1, X2 = df[pos_vars], df[pn_vars]\n",
    "    beta1, beta2 = np.array(ctrl_model['beta1']), np.array(ctrl_model['beta2'])\n",
    "    alpha = ctrl_model['alpha']\n",
    "    print(X1.shape)\n",
    "    print(beta1)\n",
    "    print(X2.shape)\n",
    "    print(beta2.shape)\n",
    "    y_pred = np.dot(X1, beta1) + np.dot(X2, beta2) + alpha\n",
    "    return y_pred\n",
    "\n",
    "base_sales = ctrl_model_predict(base_sales_model, df_ctrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "527bb2b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-07-07T21:57:46.296186Z",
     "iopub.status.busy": "2021-07-07T21:57:46.295509Z",
     "iopub.status.idle": "2021-07-07T21:57:46.317940Z",
     "shell.execute_reply": "2021-07-07T21:57:46.317308Z",
     "shell.execute_reply.started": "2021-07-07T21:57:46.296161Z"
    },
    "id": "b39208db-7b7f-478d-91ec-d44369137131",
    "outputId": "8d80a3fc-8088-41ec-aa26-4909fe3bb5d1",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (158,1) and (6000,) not aligned: 1 (dim 1) != 6000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/github/mixed-media-marketing/funcs.py\u001b[0m in \u001b[0;36mctrl_model_predict\u001b[0;34m(ctrl_model, df)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctrl_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctrl_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctrl_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (158,1) and (6000,) not aligned: 1 (dim 1) != 6000 (dim 0)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Extract the model values\n",
    "base_sales_model = funcs.extract_ctrl_model(\n",
    "    fit_ctrl, \n",
    "    pos_vars=pos_vars, \n",
    "    pn_vars=pn_vars\n",
    ")\n",
    "\n",
    "# Predict base sales\n",
    "base_sales = funcs.ctrl_model_predict(base_sales_model, df_ctrl)\n",
    "# Append back to the DF\n",
    "df['base_sales'] = base_sales*sc_ctrl['sales']\n",
    "\n",
    "# Evaluate model\n",
    "print('mape: ', funcs.mean_absolute_percentage_error(df['sales'], df['base_sales']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcec23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ceb354e-6517-4289-b124-853727c219b0",
    "outputId": "5606fb86-2216-48fc-f7fd-7a0243d2f2fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 2.2 Marketing Mix Model\n",
    "mdip_cols = [col for col in df.columns if '_imp' in col]\n",
    "mdip_cols\n",
    "\n",
    "df_mmm, sc_mmm = funcs.mean_log1p_trandform(df, ['sales', 'base_sales'])\n",
    "mu_mdip = df[mdip_cols].apply(np.mean, axis=0).values\n",
    "mu_mdip[1] = 100\n",
    "max_lag = 4\n",
    "num_media = len(mdip_cols)\n",
    "# padding zero * (max_lag-1) rows\n",
    "X_media = np.concatenate((np.zeros((max_lag-1, num_media)), df[mdip_cols].values), axis=0)\n",
    "X_media = np.nan_to_num(X_media)\n",
    "X_ctrl = df_mmm['base_sales'].values.reshape(len(df),1)\n",
    "model_data2 = {\n",
    "    'N': len(df),\n",
    "    'max_lag': max_lag, \n",
    "    'num_media': num_media,\n",
    "    'X_media': X_media, \n",
    "    'mu_mdip': mu_mdip,\n",
    "    'num_ctrl': X_ctrl.shape[1],\n",
    "    'X_ctrl': X_ctrl, \n",
    "    'y': df_mmm['sales'].values\n",
    "}\n",
    "\n",
    "model_code2 = '''\n",
    "functions {\n",
    "  // the adstock transformation with a vector of weights\n",
    "  real Adstock(vector t, row_vector weights) {\n",
    "    return dot_product(t, weights) / sum(weights);\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  // the total number of observations\n",
    "  int<lower=1> N;\n",
    "  // the vector of sales\n",
    "  real y[N];\n",
    "  // the maximum duration of lag effect, in weeks\n",
    "  int<lower=1> max_lag;\n",
    "  // the number of media channels\n",
    "  int<lower=1> num_media;\n",
    "  // matrix of media variables\n",
    "  matrix[N+max_lag-1, num_media] X_media;\n",
    "  // vector of media variables' mean\n",
    "  real mu_mdip[num_media];\n",
    "  // the number of other control variables\n",
    "  int<lower=1> num_ctrl;\n",
    "  // a matrix of control variables\n",
    "  matrix[N, num_ctrl] X_ctrl;\n",
    "}\n",
    "parameters {\n",
    "  // residual variance\n",
    "  real<lower=0> noise_var;\n",
    "  // the intercept\n",
    "  real tau;\n",
    "  // the coefficients for media variables and base sales\n",
    "  vector<lower=0>[num_media+num_ctrl] beta;\n",
    "  // the decay and peak parameter for the adstock transformation of\n",
    "  // each media\n",
    "  vector<lower=0,upper=1>[num_media] decay;\n",
    "  vector<lower=0,upper=ceil(max_lag/2)>[num_media] peak;\n",
    "}\n",
    "transformed parameters {\n",
    "  // the cumulative media effect after adstock\n",
    "  real cum_effect;\n",
    "  // matrix of media variables after adstock\n",
    "  matrix[N, num_media] X_media_adstocked;\n",
    "  // matrix of all predictors\n",
    "  matrix[N, num_media+num_ctrl] X;\n",
    "  \n",
    "  // adstock, mean-center, log1p transformation\n",
    "  row_vector[max_lag] lag_weights;\n",
    "  for (nn in 1:N) {\n",
    "    for (media in 1 : num_media) {\n",
    "      for (lag in 1 : max_lag) {\n",
    "        lag_weights[max_lag-lag+1] <- pow(decay[media], (lag - 1 - peak[media]) ^ 2);\n",
    "      }\n",
    "     cum_effect <- Adstock(sub_col(X_media, nn, media, max_lag), lag_weights);\n",
    "     X_media_adstocked[nn, media] <- log1p(cum_effect/mu_mdip[media]);\n",
    "    }\n",
    "  X <- append_col(X_media_adstocked, X_ctrl);\n",
    "  } \n",
    "}\n",
    "model {\n",
    "  decay ~ beta(3,3);\n",
    "  peak ~ uniform(0, ceil(max_lag/2));\n",
    "  tau ~ normal(0, 5);\n",
    "  for (i in 1 : num_media+num_ctrl) {\n",
    "    beta[i] ~ normal(0, 1);\n",
    "  }\n",
    "  noise_var ~ inv_gamma(0.05, 0.05 * 0.01);\n",
    "  y ~ normal(tau + X * beta, sqrt(noise_var));\n",
    "}\n",
    "'''\n",
    "print(\"Loading StanModel. . .\")\n",
    "try:\n",
    "    sm2\n",
    "except NameError:\n",
    "    sm2 = pystan.StanModel(model_code=model_code2, verbose=True)\n",
    "#sm2 = pystan.StanModel(model_code=model_code2, verbose=True)\n",
    "print(\"Beginning sampling. . .\")\n",
    "fit2 = sm2.sampling(data=model_data2, iter=1000, chains=16)\n",
    "fit2_result = fit2.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38640ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "5b836bc8-df11-41af-ad92-26e2b5f19932",
    "outputId": "c27b8220-55fa-496c-bebd-b29a66cbe555",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mmm = funcs.extract_mmm(\n",
    "    fit2, \n",
    "    max_lag=max_lag, \n",
    "    media_vars=mdip_cols, \n",
    "    ctrl_vars=['base_sales']\n",
    ")\n",
    "\n",
    "# save_json(mmm, 'mmm1.json')\n",
    "# plot media coefficients' distributions\n",
    "# red line: mean, green line: median\n",
    "beta_media = {}\n",
    "for i in range(len(mmm['media_vars'])):\n",
    "    md = mmm['media_vars'][i]\n",
    "    betas = []\n",
    "    for j in range(len(mmm['beta_list'])):\n",
    "        betas.append(mmm['beta_list'][j][i])\n",
    "    beta_media[md] = np.array(betas)\n",
    "\n",
    "f = plt.figure(figsize=(22,18))\n",
    "for i in range(len(mmm['media_vars'])):\n",
    "    ax = f.add_subplot(5,3,i+1)\n",
    "    md = mmm['media_vars'][i]\n",
    "    x = beta_media[md]\n",
    "    mean_x = x.mean()\n",
    "    median_x = np.median(x)\n",
    "    ax = sns.distplot(x)\n",
    "    ax.axvline(mean_x, color='r', linestyle='-')\n",
    "    ax.axvline(median_x, color='g', linestyle='-')\n",
    "    ax.set_title(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222225ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46da3d10-9757-4b58-82f2-c0c8700f6e79",
    "outputId": "267b9140-8c77-4b22-d1eb-aa5e71f2192b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# decompose sales to media contribution\n",
    "mc_df = funcs.mmm_decompose_contrib(mmm, df, original_sales=df['sales'])\n",
    "adstock_params = mmm['adstock_params']\n",
    "\n",
    "# calculate media contribution percentage\n",
    "mc_pct, mc_pct2 = funcs.calc_media_contrib_pct(mc_df, mdip_cols, 'sales', period=52)\n",
    "\n",
    "# mc_df.to_csv('mc_df1.csv', index=False)\n",
    "# save_json(adstock_params, 'adstock_params1.json')\n",
    "# pd.concat([\n",
    "#     pd.DataFrame.from_dict(mc_pct, orient='index', columns=['mc_pct']),\n",
    "#     pd.DataFrame.from_dict(mc_pct2, orient='index', columns=['mc_pct2'])\n",
    "# ], axis=1).to_csv('mc_pct_df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69deb2d",
   "metadata": {
    "id": "fb535add-b573-4ecd-b3fe-1195e45b320f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_code3 = '''\n",
    "functions {\n",
    "  // the Hill function\n",
    "  real Hill(real t, real ec, real slope) {\n",
    "  return 1 / (1 + (t / ec)^(-slope));\n",
    "  }\n",
    "}\n",
    "\n",
    "data {\n",
    "  // the total number of observations\n",
    "  int<lower=1> N;\n",
    "  // y: vector of media contribution\n",
    "  vector[N] y;\n",
    "  // X: vector of adstocked media spending\n",
    "  vector[N] X;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  // residual variance\n",
    "  real<lower=0> noise_var;\n",
    "  // regression coefficient\n",
    "  real<lower=0> beta_hill;\n",
    "  // ec50 and slope for Hill function of the media\n",
    "  real<lower=0,upper=1> ec;\n",
    "  real<lower=0> slope;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "  // a vector of the mean response\n",
    "  vector[N] mu;\n",
    "  for (i in 1:N) {\n",
    "    mu[i] <- beta_hill * Hill(X[i], ec, slope);\n",
    "  }\n",
    "}\n",
    "\n",
    "model {\n",
    "  slope ~ gamma(3, 1);\n",
    "  ec ~ beta(2, 2);\n",
    "  beta_hill ~ normal(0, 1);\n",
    "  noise_var ~ inv_gamma(0.05, 0.05 * 0.01); \n",
    "  y ~ normal(mu, sqrt(noise_var));\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ceab2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8e2fedf-ff2a-4ad4-a6e7-351f57c16732",
    "outputId": "ae990428-7a21-4da8-e943-681f65daf08d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# train hill models for all media channels\n",
    "try:\n",
    "    sm3\n",
    "except NameError:\n",
    "    sm3 = pystan.StanModel(model_code=model_code3, verbose=True)\n",
    "sm3 = pystan.StanModel(model_code=model_code3, verbose=True)\n",
    "hill_models = {}\n",
    "#to_train = ['dm', 'inst', 'nsp', 'auddig', 'audtr', 'vidtr', 'viddig', 'so', 'on', 'sem']\n",
    "to_train = ['ASA', 'Affiliate', 'Display', 'Email', 'PLA', \n",
    "            'Search', 'Social', 'Video', 'Facebook', 'tv']\n",
    "for media in to_train:\n",
    "    print('training for media: ', media)\n",
    "    hill_model = funcs.train_hill_model(df, mc_df, adstock_params, media, sm3)\n",
    "    print(\"trained for media: \", media)\n",
    "    hill_models[media] = hill_model\n",
    "\n",
    "# extract params by mean\n",
    "hill_model_params_mean, hill_model_params_med = {}, {}\n",
    "for md in list(hill_models.keys()):\n",
    "    print(\"extracting \" + md)\n",
    "    hill_model = hill_models[md]\n",
    "    params1 = funcs.extract_hill_model_params(hill_model, method='mean')\n",
    "    params1['sc'] = hill_model['sc']\n",
    "    hill_model_params_mean[md] = params1\n",
    "#     params2 = extract_hill_model_params(hill_model, method='median')\n",
    "#     params2['sc'] = hill_model['sc']\n",
    "#     hill_model_params_med[md] = params2\n",
    "# save_json(hill_model_params_med, 'hill_model_params_med.json')\n",
    "# save_json(hill_model_params_mean, 'hill_model_params_mean.json')\n",
    "\n",
    "# evaluate model params extracted by mean\n",
    "for md in list(hill_models.keys()):\n",
    "    print('evaluating media: ', md)\n",
    "    hill_model = hill_models[md]\n",
    "    hill_model_params = hill_model_params_mean[md]\n",
    "    _ = funcs.evaluate_hill_model(hill_model, hill_model_params)\n",
    "# evaluate model params extracted by median\n",
    "# for md in list(hill_models.keys()):\n",
    "#     print('evaluating media: ', md)\n",
    "#     hill_model = hill_models[md]\n",
    "#     hill_model_params = hill_model_params_med[md]\n",
    "#     _ = evaluate_hill_model(hill_model, hill_model_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00637457",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "8736e486-f7a6-4fa5-a2c6-2cccb1839879",
    "outputId": "e0955f0c-1a24-4889-d03a-34f22b008ef0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate overall ROAS and weekly ROAS\n",
    "# - Overall ROAS = total contribution / total spending\n",
    "# - Weekly ROAS = weekly contribution / weekly spending\n",
    "\n",
    "# adstocked media spending\n",
    "ms_df = pd.DataFrame()\n",
    "for md in list(hill_models.keys()):\n",
    "    #print(\"md: \",md)\n",
    "    hill_model = hill_models[md]\n",
    "    x = np.array(hill_model['data']['X']) * hill_model['sc']['x']\n",
    "    ms_df[md+\"_spnd\"] = x\n",
    "# ms_df.to_csv('ms_df1.csv', index=False)\n",
    "\n",
    "roas_1y = funcs.calc_roas(mc_df, ms_df, period=52)\n",
    "weekly_roas = funcs.calc_weekly_roas(mc_df, ms_df)\n",
    "roas1y_df = pd.DataFrame(index=weekly_roas.columns.tolist())\n",
    "roas1y_df['roas_mean'] = weekly_roas[-52:].apply(np.mean, axis=0)\n",
    "roas1y_df['roas_median'] = weekly_roas[-52:].apply(np.median, axis=0)\n",
    "\n",
    "# plot weekly ROAS distribution of past 1 year\n",
    "# median: green line, mean: red line\n",
    "f = plt.figure(figsize=(22,15))\n",
    "for i in range(len(weekly_roas.columns)):\n",
    "    md = weekly_roas.columns[i]\n",
    "    ax = f.add_subplot(4,3,i+1)\n",
    "    x = weekly_roas[md][-52:]\n",
    "    mean_x = np.mean(x)\n",
    "    median_x = np.median(x)\n",
    "    ax = sns.distplot(x)\n",
    "    ax.axvline(mean_x, color='r', linestyle='-', alpha=0.5)\n",
    "    ax.axvline(median_x, color='g', linestyle='-', alpha=0.5)\n",
    "    ax.set(xlabel=None)\n",
    "    ax.set_title(md)\n",
    "\n",
    "\n",
    "# Calculate mROAS\n",
    "# 1. Current spending level (cur_sp) is represented by mean or median of weekly spending.    \n",
    "# Next spending level (next_sp) is increasing cur_sp by 1%.\n",
    "# 2. Plug cur_sp and next_sp into the Hill function:    \n",
    "# Current media contribution: cur_mc = Hill(cur_sp)    \n",
    "# Next-level media contribution next_mc = Hill(next_sp)    \n",
    "# 3. mROAS = (next_mc - cur_mc) / (0.01 * cur_sp)\n",
    "\n",
    "\n",
    "# calc mROAS of recent 1 year\n",
    "mroas_1y = {}\n",
    "for md in list(hill_models.keys()):\n",
    "    hill_model = hill_models[md]\n",
    "    hill_model_params = hill_model_params_mean[md]\n",
    "    mroas_1y[md] = funcs.calc_mroas(hill_model, hill_model_params, period=52)\n",
    "\n",
    "roas1y_df = pd.concat([\n",
    "    roas1y_df[['roas_mean', 'roas_median']],\n",
    "    pd.DataFrame.from_dict(mroas_1y, orient='index', columns=['mroas']),\n",
    "    pd.DataFrame.from_dict(roas_1y, orient='index', columns=['roas_avg'])\n",
    "], axis=1)\n",
    "# roas1y_df.to_csv('roas1y_df1.csv')\n",
    "\n",
    "roas1y_df\n",
    "# **ROAS & mROAS**    \n",
    "# 'roas_avg': overall ROAS = total contribution / total spending    \n",
    "# 'roas_mean': mean of weekly ROAS\n",
    "# 'roas_median': median of weekly ROAS    \n",
    "# 'mroas': mROAS calculated based on increasing current spending level by 1%   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c4fc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 966
    },
    "id": "vYoFEyPY9ef0",
    "outputId": "4b4eadb2-4f82-4f4f-9c8a-8252c8b665cb"
   },
   "outputs": [],
   "source": [
    "recent_dates = df[-52:-2]['wk_strt_dt']\n",
    "\n",
    "f = plt.figure(figsize=(22,15))\n",
    "for i in range(len(weekly_roas.columns)):\n",
    "    md = weekly_roas.columns[i]\n",
    "    print(md)\n",
    "    ax = f.add_subplot(4,3,i+1)\n",
    "    x = weekly_roas[md][-52:-2]\n",
    "    print(x.min())\n",
    "    mean_x = np.mean(x)\n",
    "    median_x = np.median(x)\n",
    "    ax = sns.lineplot(x=list(range(0,50)),y=x)\n",
    "    #ax.axvline(mean_x, color='r', linestyle='-', alpha=0.5)\n",
    "    #ax.axvline(median_x, color='g', linestyle='-', alpha=0.5)\n",
    "    ax.set(xlabel=None)\n",
    "    ax.set_title(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2a681",
   "metadata": {
    "id": "WvUSu3dXGeFZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "STAN_GOOD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
